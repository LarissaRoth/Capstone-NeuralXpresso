{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os           \n",
    "\n",
    "# Check if string is appropriate youtube link\n",
    "import re\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from facenet_pytorch import MTCNN\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import load_model\n",
    "from pytube import YouTube\n",
    "\n",
    "import face_recognition\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "SAVE_VIDEO = False\n",
    "SHOW_PROMPT = False\n",
    "\n",
    "# DEBUG-Mode stops operations when max_emotions were detected and also prints a short summary\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    import time\n",
    "    debug_params = {\n",
    "        'max_emotions' : 1000\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(kind):\n",
    "    if kind == \"youtube\":\n",
    "        return \"https://www.youtube.com/watch?v=vtT78TfDfXU\"                   # Random video\n",
    "        #return 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "        #return 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "    if kind == \"local\":\n",
    "        # adjust individually\n",
    "        return '/Users/steve/Neue_Fische/face_demo/vids/Video_One_output.mp4'\n",
    "    if kind == \"error_on_purpose\":\n",
    "        return \"wrongful path\"\n",
    "    else:\n",
    "        raise ValueError(f\"Passed Argument kind must bei in ['youtube', 'local', 'error_on_purpose'] but was: {kind}\")\n",
    "\n",
    "def youtube_stream(yt_link):\n",
    "    # Load the video from YouTube\n",
    "    yt_video = YouTube(yt_link)\n",
    "    stream = yt_video.streams.get_highest_resolution()    #or highest resolution?\n",
    "    return cv2.VideoCapture(stream.url)\n",
    "\n",
    "def local_stream(local_path):\n",
    "    return cv2.VideoCapture(local_path)\n",
    "\n",
    "def get_stream(path):\n",
    "    # Check if the string is a YouTube link\n",
    "    if re.match(r'(https?://)?(www\\.)?(youtube\\.com|youtu\\.?be)/.+$', path):\n",
    "        return youtube_stream(path)\n",
    "    # Check if the string is a local path\n",
    "    elif os.path.isfile(path):\n",
    "        return local_stream(path)\n",
    "    # Check if the path is a local file path but no file is found\n",
    "    elif os.path.exists(path):\n",
    "        raise ValueError(f\"File not found at path: {path}\")\n",
    "    # If it's neither a local path nor a YouTube link, raise an error\n",
    "    else:\n",
    "        raise ValueError(\"The input string is neither a local path nor a YouTube link.\")\n",
    "    \n",
    "def load_emotion_classifier():\n",
    "    return load_model(\"../models/emotion_model.hdf5\", compile=False)\n",
    "\n",
    "def preprocess_face(face, input_face_size):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_RGB2GRAY)  # Convert the face to grayscale\n",
    "    face = cv2.resize(face, (input_face_size[1], input_face_size[0]))  # Swap width and height\n",
    "    face = face.astype('float32') / 255.0\n",
    "    face = np.expand_dims(face, axis=-1)  # Add an additional dimension for grayscale channel\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    return face\n",
    "\n",
    "def print_debug_report(operating_results):    \n",
    "    print(f'{operating_results[\"analyzed_emotions\"]} faces found in {operating_results[\"analyzed_frames\"]} frames.')\n",
    "    print(f'{operating_results[\"frames_without_faces\"]} frames had no face detected ({operating_results[\"frames_without_faces_ratio\"]}%).')\n",
    "    print(f'Stopped operations after around {operating_results[\"processed_video_time\"]} seconds into the video.')\n",
    "    print(f'Execution time: {operating_results[\"runtime\"]} seconds, processing (roughly) {round(operating_results[\"processed_video_time\"]/operating_results[\"runtime\"],2)} seconds of video per second of execution')\n",
    "\n",
    "def initialize_face_detector(model_type):\n",
    "    if model_type == 'haarcascade':\n",
    "        return cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    elif model_type == \"MTCNN\":\n",
    "        return MTCNN(keep_all=True, post_process=False, margin=20)\n",
    "    else:\n",
    "        raise ValueError(\"By now, only Haarcascade is implemented.\")\n",
    "\n",
    "def preprocess_frame_for_face_detection_haarcascade(frame):\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def preprocess_frame_for_emotion_detection(frame):\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def normalize_boxes_mtcnn(boxes):\n",
    "    \"\"\"\n",
    "    Normalize the bounding box coordinates from MTCNN to numpy indexing format.\n",
    "    Output format: np.array(y_min, y_max, x_min, x_max)\n",
    "    \"\"\"\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box.astype(int)\n",
    "        normalized_boxes.append([y_min, y_max, x_min, x_max])\n",
    "    return np.array(normalized_boxes)\n",
    "\n",
    "def normalize_boxes_cv2(boxes):\n",
    "    \"\"\"\n",
    "    Normalize the bounding box coordinates from OpenCV's format to numpy indexing format.\n",
    "    Output format: np.array(y_min, y_max, x_min, x_max)\n",
    "    \"\"\"\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        normalized_boxes.append([y, y+h, x, x+w])\n",
    "    return np.array(normalized_boxes)\n",
    "\n",
    "def detect_faces(frame, face_detector, model_type = 'haarcascade'):\n",
    "    if model_type == 'haarcascade':\n",
    "        frame_pp = preprocess_frame_for_face_detection_haarcascade(frame)\n",
    "        boxes =  face_detector.detectMultiScale(frame_pp, scaleFactor = 1.3, minNeighbors = 3)\n",
    "        if boxes is None:\n",
    "            return None\n",
    "        else:\n",
    "            return normalize_boxes_cv2(boxes)\n",
    "    elif model_type == \"MTCNN\":\n",
    "        # No preprocessing needed for VideoCapture Frame\n",
    "        boxes, _ = face_detector.detect(frame)\n",
    "        if boxes is None:\n",
    "            return None\n",
    "        else:\n",
    "            return normalize_boxes_mtcnn(boxes)\n",
    "        #return [(x['box'][1], x['box'][0] + x['box'][2], x['box'][1] + x['box'][3], x['box'][0]) for x in bounding_boxes]\n",
    "    #elif model_type == \"fast_MTCNN\"\n",
    "    # https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49\n",
    "    else:\n",
    "        raise ValueError(\"By now, only Haarcascade is implemented.\")\n",
    "    \n",
    "def get_ordered_emotions():\n",
    "    return ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "def emotions_probability(frame, face_location, emotion_classifier):\n",
    "    frame_pp = preprocess_frame_for_emotion_detection(frame)\n",
    "    y_min, y_max, x_min, x_max = face_location\n",
    "    face = frame_pp[y_min:y_max, x_min:x_max]\n",
    "    face = preprocess_face(face, input_face_size=emotion_classifier.input_shape[1:3])\n",
    "    prob = emotion_classifier.predict(face)[0]  # check for underscore\n",
    "    return prob\n",
    "\n",
    "def output_video(video, filename):\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    return cv2.VideoWriter(filename, fourcc, 10, (width,height))\n",
    "\n",
    "def get_overview_df(emotions, frame_info, frame_info_cols):\n",
    "    assert len(frame_info[0]) == len(frame_info_cols), \\\n",
    "        f\"Number of columns in frame_info and number of passed names in frame_info_cols is not the same: {len(frame_info[0])} != {len(frame_info_cols)}.\"\n",
    "\n",
    "    df_emotions = pd.DataFrame(emotions, columns=get_ordered_emotions())\n",
    "    df_frame_info = pd.DataFrame(frame_info, columns=frame_info_cols)\n",
    "    df_all_info = pd.concat([df_emotions, df_frame_info], axis=1)\n",
    "    return df_all_info\n",
    "\n",
    "def get_plottable_df(emotions, frame_info, frame_info_cols):\n",
    "\n",
    "    assert len(frame_info[0]) == len(frame_info_cols), \\\n",
    "        f\"Number of columns in frame_info and number of passed names in frame_info_cols is not the same: {len(frame_info[0])} != {len(frame_info_cols)}.\"\n",
    "\n",
    "    df_emotions = pd.DataFrame(emotions, columns=get_ordered_emotions())\n",
    "    df_frame_info = pd.DataFrame(frame_info, columns=frame_info_cols)\n",
    "    df_all_info = pd.concat([df_emotions, df_frame_info], axis=1)\n",
    "    df_plotting = pd.melt(df_all_info, id_vars=frame_info_cols, value_vars=get_ordered_emotions(), var_name='emotion', value_name='probability')\n",
    "    return df_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tls @ 0x177112940] Error in the pull function.\n",
      "[tls @ 0x177112940] IO error: Connection reset by peer\n",
      "[NULL @ 0x288075b70] Invalid NAL unit size (546 > 300).\n",
      "[NULL @ 0x288075b70] missing picture in access unit with size 304\n",
      "[h264 @ 0x28809d5d0] Invalid NAL unit size (546 > 300).\n",
      "[h264 @ 0x28809d5d0] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x1771184a0] stream 0, offset 0x563d2a: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x1771184a0] stream 0, offset 0x564d85: partial file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'video': {'available_resolutions': ['144p', '360p', '720p'],\n",
       "  'used_resolution': '720p',\n",
       "  'fps_pytube': 24,\n",
       "  'fps_cv2': 24.0,\n",
       "  'framecount': 1785.0,\n",
       "  'length_s': 74.375},\n",
       " 'models': {'MTCNN': {'model_name': 'MTCNN',\n",
       "   'initialization_time': 0.0097,\n",
       "   'frames_analyzed': 1651,\n",
       "   'processing_time': 171.2695279121399,\n",
       "   'processed_s_video_per_s_runtime': 0.4343,\n",
       "   'nr_frames_no_face_found': 234,\n",
       "   'ratio_frames_no_face_found': 0.142,\n",
       "   'nr_frames_one_face_found': 1415,\n",
       "   'ratio_frames_one_face_found': 0.857,\n",
       "   'nr_frames_multiple_faces_found': 2,\n",
       "   'ratio_frames_multiple_faces_found': 0.001},\n",
       "  'haarcascade': {'model_name': 'haarcascade',\n",
       "   'initialization_time': 0.0239,\n",
       "   'frames_analyzed': 1785,\n",
       "   'processing_time': 22.80318808555603,\n",
       "   'processed_s_video_per_s_runtime': 3.2616,\n",
       "   'nr_frames_no_face_found': 619,\n",
       "   'ratio_frames_no_face_found': 0.347,\n",
       "   'nr_frames_one_face_found': 1153,\n",
       "   'ratio_frames_one_face_found': 0.646,\n",
       "   'nr_frames_multiple_faces_found': 13,\n",
       "   'ratio_frames_multiple_faces_found': 0.007}}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "yt_link = 'https://www.youtube.com/watch?v=vtT78TfDfXU'                   # 1 Actor\n",
    "#yt_link = 'https://www.youtube.com/watch?v=embYkODkzcs'                 # 7 basic emotions\n",
    "#yt_link = 'https://www.youtube.com/watch?v=m70UInZKJjU'                    # Two persons\n",
    "\n",
    "yt_video = YouTube(yt_link)\n",
    "stream = yt_video.streams.get_highest_resolution()    #or highest resolution?\n",
    "video =  cv2.VideoCapture(stream.url)\n",
    "model_evaluation = {'video':{}, 'models': {}}\n",
    "\n",
    "# Initialize writer to save the annotated video\n",
    "if SAVE_VIDEO: writer = output_video(video, filename='outputs/Output_video.mp4')\n",
    "\n",
    "model_evaluation['video'] = {'available_resolutions': [streams.resolution for streams in yt_video.streams.filter(type=\"video\", progressive=True)],\n",
    "                             'used_resolution' : stream.resolution,\n",
    "                             'fps_pytube': stream.fps,\n",
    "                             'fps_cv2': video.get(cv2.CAP_PROP_FPS),\n",
    "                             'framecount': video.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "                             'length_s': round(video.get(cv2.CAP_PROP_FRAME_COUNT) / video.get(cv2.CAP_PROP_FPS),4)\n",
    "                             }\n",
    "\n",
    "for model_type in[\"MTCNN\", \"haarcascade\"]:\n",
    "\n",
    "    video =  cv2.VideoCapture(stream.url)\n",
    "\n",
    "    start_time_initialization = time.time()\n",
    "    # Initialize the face detection model\n",
    "\n",
    "    face_detector = initialize_face_detector(model_type)\n",
    "    end_time_initialization = time.time()\n",
    "\n",
    "    frame_info = []\n",
    "    faces_found = 0\n",
    "\n",
    "    model_evaluation['models'][model_type] = {'model_name': model_type,\n",
    "                                            'initialization_time': round(end_time_initialization-start_time_initialization,4)\n",
    "                                            }\n",
    "\n",
    "    start_time_loop = time.time()\n",
    "    # Loop through each frame of the video\n",
    "    while True:\n",
    "\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        # Check if the frame was successfully read\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Increment the frame counter\n",
    "        current_frame_nr = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        \n",
    "        # Find faces within a frame and return list of coordinates of bounding boxes\n",
    "        face_locations = detect_faces(frame, face_detector, model_type)\n",
    "\n",
    "        # Check if any faces were found\n",
    "        if face_locations is None:\n",
    "            faces_found = 0\n",
    "        else:\n",
    "            faces_found = len(face_locations)\n",
    "                    \n",
    "        frame_info.append(\n",
    "            (model_type, round(video.get(cv2.CAP_PROP_POS_MSEC) / 1000, 2), current_frame_nr, faces_found ))    \n",
    "\n",
    "            #for (y_min, y_max, x_min, x_max) in face_locations:\n",
    "            #   cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            #  cv2.putText(frame, f\"Frame {current_frame_nr}\", (30,40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 1)\n",
    "\n",
    "        if DEBUG:\n",
    "            # For debugging reasons, we stop when we have 1000 emotion values\n",
    "            if len(frame_info) > debug_params['max_emotions']:\n",
    "                break\n",
    "\n",
    "    end_time_loop = time.time()\n",
    "\n",
    "    # Release the video and close the window\n",
    "    video.release()\n",
    "\n",
    "    frame_info_array = np.array(frame_info)\n",
    "\n",
    "    model_evaluation['models'][model_type]['frames_analyzed'] = len(frame_info)\n",
    "    model_evaluation['models'][model_type]['processing_time'] = end_time_loop-start_time_loop\n",
    "    model_evaluation['models'][model_type]['processed_s_video_per_s_runtime'] = round(model_evaluation['video']['length_s']/model_evaluation['models'][model_type]['processing_time'],4)\n",
    "    model_evaluation['models'][model_type]['nr_frames_no_face_found']=np.sum(np.equal(frame_info_array[:, 3].astype(int),0))\n",
    "    model_evaluation['models'][model_type]['ratio_frames_no_face_found']=round(model_evaluation['models'][model_type]['nr_frames_no_face_found']/model_evaluation['models'][model_type]['frames_analyzed'],3)\n",
    "    model_evaluation['models'][model_type]['nr_frames_one_face_found']=np.sum(np.equal(frame_info_array[:, 3].astype(int),1))\n",
    "    model_evaluation['models'][model_type]['ratio_frames_one_face_found']=round(model_evaluation['models'][model_type]['nr_frames_one_face_found']/model_evaluation['models'][model_type]['frames_analyzed'],3)\n",
    "    model_evaluation['models'][model_type]['nr_frames_multiple_faces_found']=np.sum(np.greater(frame_info_array[:, 3].astype(int),1))\n",
    "    model_evaluation['models'][model_type]['ratio_frames_multiple_faces_found']=round(model_evaluation['models'][model_type]['nr_frames_multiple_faces_found']/model_evaluation['models'][model_type]['frames_analyzed'],3)\n",
    "\n",
    "model_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['haarcascade', '0.0', '1', '1'],\n",
       "       ['haarcascade', '0.04', '2', '1'],\n",
       "       ['haarcascade', '0.08', '3', '1'],\n",
       "       ...,\n",
       "       ['haarcascade', '74.25', '1783', '1'],\n",
       "       ['haarcascade', '74.29', '1784', '1'],\n",
       "       ['haarcascade', '74.33', '1785', '1']], dtype='<U32')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_info_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "292d9171ab7c5408b6324c6043caf24aa9a11213294e6785916c17bd69f18dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
